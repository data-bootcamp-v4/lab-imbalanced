{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with Credit Card Fraud dataset.\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "- **distance_from_home:** the distance from home where the transaction happened.\n",
    "- **distance_from_last_transaction:** the distance from last transaction happened.\n",
    "- **ratio_to_median_purchase_price:** Ratio of purchased price transaction to median purchase price.\n",
    "- **repeat_retailer:** Is the transaction happened from same retailer.\n",
    "- **used_chip:** Is the transaction through chip (credit card).\n",
    "- **used_pin_number:** Is the transaction happened by using PIN number.\n",
    "- **online_order:** Is the transaction an online order.\n",
    "- **fraud:** Is the transaction fraudulent. **0=legit** -  **1=fraud**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2.207101</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>1.626798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>19.872726</td>\n",
       "      <td>2.683904</td>\n",
       "      <td>2.778303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2.914857</td>\n",
       "      <td>1.472687</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>4.258729</td>\n",
       "      <td>0.242023</td>\n",
       "      <td>0.475822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>58.108125</td>\n",
       "      <td>0.318110</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "0                57.877857                        0.311140   \n",
       "1                10.829943                        0.175592   \n",
       "2                 5.091079                        0.805153   \n",
       "3                 2.247564                        5.600044   \n",
       "4                44.190936                        0.566486   \n",
       "...                    ...                             ...   \n",
       "999995            2.207101                        0.112651   \n",
       "999996           19.872726                        2.683904   \n",
       "999997            2.914857                        1.472687   \n",
       "999998            4.258729                        0.242023   \n",
       "999999           58.108125                        0.318110   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                             1.945940              1.0        1.0   \n",
       "1                             1.294219              1.0        0.0   \n",
       "2                             0.427715              1.0        0.0   \n",
       "3                             0.362663              1.0        1.0   \n",
       "4                             2.222767              1.0        1.0   \n",
       "...                                ...              ...        ...   \n",
       "999995                        1.626798              1.0        1.0   \n",
       "999996                        2.778303              1.0        1.0   \n",
       "999997                        0.218075              1.0        1.0   \n",
       "999998                        0.475822              1.0        0.0   \n",
       "999999                        0.386920              1.0        1.0   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "0                   0.0           0.0    0.0  \n",
       "1                   0.0           0.0    0.0  \n",
       "2                   0.0           1.0    0.0  \n",
       "3                   0.0           1.0    0.0  \n",
       "4                   0.0           1.0    0.0  \n",
       "...                 ...           ...    ...  \n",
       "999995              0.0           0.0    0.0  \n",
       "999996              0.0           0.0    0.0  \n",
       "999997              0.0           1.0    0.0  \n",
       "999998              0.0           1.0    0.0  \n",
       "999999              0.0           1.0    0.0  \n",
       "\n",
       "[1000000 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\")\n",
    "fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "- **2.** Train a LogisticRegression.\n",
    "- **3.** Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "- **4.** Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "- **5.** Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "- **6.** Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fraud\n",
       "0.0    912597\n",
       "1.0     87403\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting the distribution of the 'fraud' column\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='fraud', data=fraud)\n",
    "plt.title('Distribution of Fraud Column')\n",
    "plt.xlabel('Fraud')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Displaying the value counts for additional insight\n",
    "fraud_counts = fraud['fraud'].value_counts()\n",
    "fraud_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = fraud.drop('fraud', axis=1)\n",
    "y = fraud['fraud']\n",
    "\n",
    "# Identify the numerical columns\n",
    "numerical_cols = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numerical columns\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506532</th>\n",
       "      <td>-0.242599</td>\n",
       "      <td>-0.137685</td>\n",
       "      <td>-0.576500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327991</th>\n",
       "      <td>-0.362928</td>\n",
       "      <td>-0.191469</td>\n",
       "      <td>1.248963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715937</th>\n",
       "      <td>-0.351508</td>\n",
       "      <td>-0.179028</td>\n",
       "      <td>0.044170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860726</th>\n",
       "      <td>-0.208932</td>\n",
       "      <td>0.124909</td>\n",
       "      <td>-0.595091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78617</th>\n",
       "      <td>-0.326121</td>\n",
       "      <td>-0.133889</td>\n",
       "      <td>-0.533174</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>-0.398974</td>\n",
       "      <td>-0.060248</td>\n",
       "      <td>-0.605334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365838</th>\n",
       "      <td>1.333129</td>\n",
       "      <td>-0.188502</td>\n",
       "      <td>-0.107355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>-0.094131</td>\n",
       "      <td>-0.194719</td>\n",
       "      <td>-0.324696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>-0.254928</td>\n",
       "      <td>-0.188387</td>\n",
       "      <td>-0.232201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>-0.396457</td>\n",
       "      <td>-0.097842</td>\n",
       "      <td>-0.164897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "506532           -0.242599                       -0.137685   \n",
       "327991           -0.362928                       -0.191469   \n",
       "715937           -0.351508                       -0.179028   \n",
       "860726           -0.208932                        0.124909   \n",
       "78617            -0.326121                       -0.133889   \n",
       "...                    ...                             ...   \n",
       "259178           -0.398974                       -0.060248   \n",
       "365838            1.333129                       -0.188502   \n",
       "131932           -0.094131                       -0.194719   \n",
       "671155           -0.254928                       -0.188387   \n",
       "121958           -0.396457                       -0.097842   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "506532                       -0.576500              1.0        0.0   \n",
       "327991                        1.248963              1.0        0.0   \n",
       "715937                        0.044170              1.0        0.0   \n",
       "860726                       -0.595091              1.0        1.0   \n",
       "78617                        -0.533174              1.0        0.0   \n",
       "...                                ...              ...        ...   \n",
       "259178                       -0.605334              0.0        0.0   \n",
       "365838                       -0.107355              1.0        0.0   \n",
       "131932                       -0.324696              1.0        0.0   \n",
       "671155                       -0.232201              1.0        0.0   \n",
       "121958                       -0.164897              0.0        1.0   \n",
       "\n",
       "        used_pin_number  online_order  \n",
       "506532              0.0           0.0  \n",
       "327991              0.0           1.0  \n",
       "715937              0.0           1.0  \n",
       "860726              0.0           1.0  \n",
       "78617               0.0           0.0  \n",
       "...                 ...           ...  \n",
       "259178              0.0           0.0  \n",
       "365838              0.0           0.0  \n",
       "131932              0.0           0.0  \n",
       "671155              0.0           1.0  \n",
       "121958              0.0           0.0  \n",
       "\n",
       "[700000 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95849\n",
      "Confusion Matrix:\n",
      " [[271912   1959]\n",
      " [ 10494  15635]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98    273871\n",
      "         1.0       0.89      0.60      0.72     26129\n",
      "\n",
      "    accuracy                           0.96    300000\n",
      "   macro avg       0.93      0.80      0.85    300000\n",
      "weighted avg       0.96      0.96      0.95    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that the model correctly identifies the majority of non-fraudulent transactions (TN) and has a relatively lower performance in identifying fraudulent transactions (TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93418\n",
      "Confusion Matrix:\n",
      " [[255412  18459]\n",
      " [  1287  24842]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96    273871\n",
      "         1.0       0.57      0.95      0.72     26129\n",
      "\n",
      "    accuracy                           0.93    300000\n",
      "   macro avg       0.78      0.94      0.84    300000\n",
      "weighted avg       0.96      0.93      0.94    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_resampled = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the resampled model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_resampled))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_resampled))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling (SMOTE) has effectively balanced the dataset, resulting in a model that shows improved performance in detecting fraudulent transactions. Despite a slight decrease in overall accuracy, the model's ability to identify fraudulent transactions (as indicated by higher recall and a reasonable F1-score) has significantly improved. This balanced approach is crucial in scenarios where the minority class (fraud) is of particular interest and requires adequate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9336866666666667\n",
      "Confusion Matrix:\n",
      " [[255238  18633]\n",
      " [  1261  24868]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96    273871\n",
      "         1.0       0.57      0.95      0.71     26129\n",
      "\n",
      "    accuracy                           0.93    300000\n",
      "   macro avg       0.78      0.94      0.84    300000\n",
      "weighted avg       0.96      0.93      0.94    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numerical columns\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Apply undersampling to the training data\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model on the resampled data\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_resampled = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the resampled model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_resampled))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_resampled))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling has effectively balanced the dataset, resulting in a model that shows improved performance in detecting fraudulent transactions. The model's ability to identify fraudulent transactions (as indicated by higher recall and a reasonable precision) has significantly improved compared to the unbalanced dataset. While undersampling has slightly reduced the overall accuracy, it has provided a more balanced view of model performance across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
