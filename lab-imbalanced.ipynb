{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with Credit Card Fraud dataset.\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "- **distance_from_home:** the distance from home where the transaction happened.\n",
    "- **distance_from_last_transaction:** the distance from last transaction happened.\n",
    "- **ratio_to_median_purchase_price:** Ratio of purchased price transaction to median purchase price.\n",
    "- **repeat_retailer:** Is the transaction happened from same retailer.\n",
    "- **used_chip:** Is the transaction through chip (credit card).\n",
    "- **used_pin_number:** Is the transaction happened by using PIN number.\n",
    "- **online_order:** Is the transaction an online order.\n",
    "- **fraud:** Is the transaction fraudulent. **0=legit** -  **1=fraud**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.5.0\n",
      "Uninstalling scikit-learn-1.5.0:\n",
      "  Successfully uninstalled scikit-learn-1.5.0\n",
      "Found existing installation: imbalanced-learn 0.12.2\n",
      "Uninstalling imbalanced-learn-0.12.2:\n",
      "  Successfully uninstalled imbalanced-learn-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.24.2\n",
      "  Downloading scikit_learn-0.24.2-cp39-cp39-macosx_10_13_x86_64.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2) (3.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2) (1.4.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2) (1.20.3)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.24.2\n",
      "Collecting imbalanced-learn==0.8.0\n",
      "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/juanfransf/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn==0.8.0) (3.5.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.24.2\n",
    "!pip install imbalanced-learn==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940              1.0        1.0   \n",
       "1                        1.294219              1.0        0.0   \n",
       "2                        0.427715              1.0        0.0   \n",
       "3                        0.362663              1.0        1.0   \n",
       "4                        2.222767              1.0        1.0   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0              0.0           0.0    0.0  \n",
       "1              0.0           0.0    0.0  \n",
       "2              0.0           1.0    0.0  \n",
       "3              0.0           1.0    0.0  \n",
       "4              0.0           1.0    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\")\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "- **2.** Train a LogisticRegression.\n",
    "- **3.** Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "- **4.** Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "- **5.** Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "- **6.** Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    912597\n",
      "1.0     87403\n",
      "Name: fraud, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "\n",
    "print(fraud['fraud'].value_counts())\n",
    "\n",
    "# Visualización de la distribución\n",
    "plt.figure(figsize=(6,4))\n",
    "fraud['fraud'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribución de la variable objetivo')\n",
    "plt.xlabel('Fraud')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Train a LogisticRegression.\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = fraud.drop(columns=['fraud'])\n",
    "y = fraud['fraud']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar el modelo de Regresión Logística\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98    182557\n",
      "         1.0       0.89      0.60      0.72     17443\n",
      "\n",
      "    accuracy                           0.96    200000\n",
      "   macro avg       0.93      0.80      0.85    200000\n",
      "weighted avg       0.96      0.96      0.96    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3. Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "# Evaluar el modelo\n",
    "log_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred=pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780419</th>\n",
       "      <td>0.890714</td>\n",
       "      <td>-0.074172</td>\n",
       "      <td>1.172600</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498345</th>\n",
       "      <td>2.442871</td>\n",
       "      <td>-0.177207</td>\n",
       "      <td>2.516135</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487498</th>\n",
       "      <td>-0.228100</td>\n",
       "      <td>-0.182480</td>\n",
       "      <td>0.868001</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525521</th>\n",
       "      <td>1.345261</td>\n",
       "      <td>-0.175402</td>\n",
       "      <td>0.138045</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244719</th>\n",
       "      <td>-0.332531</td>\n",
       "      <td>-0.089242</td>\n",
       "      <td>1.284493</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>-0.399916</td>\n",
       "      <td>-0.058666</td>\n",
       "      <td>-0.606761</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>1.336886</td>\n",
       "      <td>-0.184330</td>\n",
       "      <td>-0.107909</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>-0.094246</td>\n",
       "      <td>-0.190421</td>\n",
       "      <td>-0.325631</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>-0.255480</td>\n",
       "      <td>-0.184217</td>\n",
       "      <td>-0.232974</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>-0.397393</td>\n",
       "      <td>-0.095500</td>\n",
       "      <td>-0.165552</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460080 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "780419            0.890714                       -0.074172   \n",
       "498345            2.442871                       -0.177207   \n",
       "487498           -0.228100                       -0.182480   \n",
       "525521            1.345261                       -0.175402   \n",
       "244719           -0.332531                       -0.089242   \n",
       "...                    ...                             ...   \n",
       "799995           -0.399916                       -0.058666   \n",
       "799996            1.336886                       -0.184330   \n",
       "799997           -0.094246                       -0.190421   \n",
       "799998           -0.255480                       -0.184217   \n",
       "799999           -0.397393                       -0.095500   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "780419                        1.172600         0.366618  -0.734374   \n",
       "498345                        2.516135         0.366618  -0.734374   \n",
       "487498                        0.868001         0.366618   1.361704   \n",
       "525521                        0.138045         0.366618  -0.734374   \n",
       "244719                        1.284493         0.366618  -0.734374   \n",
       "...                                ...              ...        ...   \n",
       "799995                       -0.606761        -2.727632  -0.734374   \n",
       "799996                       -0.107909         0.366618  -0.734374   \n",
       "799997                       -0.325631         0.366618  -0.734374   \n",
       "799998                       -0.232974         0.366618  -0.734374   \n",
       "799999                       -0.165552        -2.727632   1.361704   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "780419        -0.334593      0.732884    1.0  \n",
       "498345        -0.334593      0.732884    1.0  \n",
       "487498        -0.334593      0.732884    1.0  \n",
       "525521        -0.334593      0.732884    1.0  \n",
       "244719        -0.334593      0.732884    1.0  \n",
       "...                 ...           ...    ...  \n",
       "799995        -0.334593     -1.364472    0.0  \n",
       "799996        -0.334593     -1.364472    0.0  \n",
       "799997        -0.334593     -1.364472    0.0  \n",
       "799998        -0.334593      0.732884    0.0  \n",
       "799999        -0.334593     -1.364472    0.0  \n",
       "\n",
       "[1460080 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "train = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "train[\"fraud\"] = y_train.values\n",
    "\n",
    "fraud = train[train[\"fraud\"] == 1]\n",
    "no_fraud = train[train[\"fraud\"] == 0]\n",
    "\n",
    "\n",
    "fraud_oversampled = resample(fraud, \n",
    "                                    replace=True, \n",
    "                                    n_samples = len(no_fraud),\n",
    "                                    random_state=0)\n",
    "\n",
    "train_over = pd.concat([fraud_oversampled, no_fraud])\n",
    "train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "survived_plt = train_over[\"fraud\"].value_counts()\n",
    "survived_plt.plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, with balanced data, we will create a new instance of Logistic Regression.\n",
    "X_train_over = train_over.drop(columns = [\"fraud\"])\n",
    "y_train_over = train_over[\"fraud\"]\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_over, y_train_over)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96    182557\n",
      "         1.0       0.58      0.95      0.72     17443\n",
      "\n",
      "    accuracy                           0.93    200000\n",
      "   macro avg       0.79      0.94      0.84    200000\n",
      "weighted avg       0.96      0.93      0.94    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147170</td>\n",
       "      <td>-0.181191</td>\n",
       "      <td>-0.588185</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.091174</td>\n",
       "      <td>-0.168859</td>\n",
       "      <td>-0.415653</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.334813</td>\n",
       "      <td>-0.100009</td>\n",
       "      <td>0.612860</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.317543</td>\n",
       "      <td>-0.143624</td>\n",
       "      <td>-0.551591</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.077164</td>\n",
       "      <td>-0.126583</td>\n",
       "      <td>-0.590736</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>-0.399916</td>\n",
       "      <td>-0.058666</td>\n",
       "      <td>-0.606761</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>1.336886</td>\n",
       "      <td>-0.184330</td>\n",
       "      <td>-0.107909</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>-0.094246</td>\n",
       "      <td>-0.190421</td>\n",
       "      <td>-0.325631</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>-0.255480</td>\n",
       "      <td>-0.184217</td>\n",
       "      <td>-0.232974</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>-0.397393</td>\n",
       "      <td>-0.095500</td>\n",
       "      <td>-0.165552</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "0                 0.147170                       -0.181191   \n",
       "1                -0.091174                       -0.168859   \n",
       "2                -0.334813                       -0.100009   \n",
       "3                -0.317543                       -0.143624   \n",
       "4                 0.077164                       -0.126583   \n",
       "...                    ...                             ...   \n",
       "799995           -0.399916                       -0.058666   \n",
       "799996            1.336886                       -0.184330   \n",
       "799997           -0.094246                       -0.190421   \n",
       "799998           -0.255480                       -0.184217   \n",
       "799999           -0.397393                       -0.095500   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                            -0.588185         0.366618   1.361704   \n",
       "1                            -0.415653         0.366618   1.361704   \n",
       "2                             0.612860         0.366618   1.361704   \n",
       "3                            -0.551591         0.366618  -0.734374   \n",
       "4                            -0.590736         0.366618  -0.734374   \n",
       "...                                ...              ...        ...   \n",
       "799995                       -0.606761        -2.727632  -0.734374   \n",
       "799996                       -0.107909         0.366618  -0.734374   \n",
       "799997                       -0.325631         0.366618  -0.734374   \n",
       "799998                       -0.232974         0.366618  -0.734374   \n",
       "799999                       -0.165552        -2.727632   1.361704   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "0             -0.334593      0.732884    0.0  \n",
       "1             -0.334593      0.732884    0.0  \n",
       "2             -0.334593      0.732884    0.0  \n",
       "3             -0.334593      0.732884    0.0  \n",
       "4             -0.334593     -1.364472    0.0  \n",
       "...                 ...           ...    ...  \n",
       "799995        -0.334593     -1.364472    0.0  \n",
       "799996        -0.334593     -1.364472    0.0  \n",
       "799997        -0.334593     -1.364472    0.0  \n",
       "799998        -0.334593      0.732884    0.0  \n",
       "799999        -0.334593     -1.364472    0.0  \n",
       "\n",
       "[800000 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334972</th>\n",
       "      <td>-0.324630</td>\n",
       "      <td>0.202841</td>\n",
       "      <td>-0.082387</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477554</th>\n",
       "      <td>-0.255016</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.251904</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129226</th>\n",
       "      <td>-0.384474</td>\n",
       "      <td>-0.131608</td>\n",
       "      <td>-0.588554</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166812</th>\n",
       "      <td>0.102985</td>\n",
       "      <td>-0.182768</td>\n",
       "      <td>-0.634326</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394209</th>\n",
       "      <td>-0.339862</td>\n",
       "      <td>-0.186961</td>\n",
       "      <td>-0.565600</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509444</th>\n",
       "      <td>-0.331600</td>\n",
       "      <td>-0.179853</td>\n",
       "      <td>-0.053064</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389557</th>\n",
       "      <td>-0.353789</td>\n",
       "      <td>1.007705</td>\n",
       "      <td>-0.398868</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582253</th>\n",
       "      <td>-0.291767</td>\n",
       "      <td>-0.158301</td>\n",
       "      <td>-0.226416</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670183</th>\n",
       "      <td>-0.348281</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>-0.164511</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257272</th>\n",
       "      <td>-0.076356</td>\n",
       "      <td>-0.113615</td>\n",
       "      <td>-0.285170</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69960 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "334972           -0.324630                        0.202841   \n",
       "477554           -0.255016                       -0.136678   \n",
       "129226           -0.384474                       -0.131608   \n",
       "166812            0.102985                       -0.182768   \n",
       "394209           -0.339862                       -0.186961   \n",
       "...                    ...                             ...   \n",
       "509444           -0.331600                       -0.179853   \n",
       "389557           -0.353789                        1.007705   \n",
       "582253           -0.291767                       -0.158301   \n",
       "670183           -0.348281                        0.049960   \n",
       "257272           -0.076356                       -0.113615   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "334972                       -0.082387         0.366618  -0.734374   \n",
       "477554                       -0.251904         0.366618   1.361704   \n",
       "129226                       -0.588554        -2.727632  -0.734374   \n",
       "166812                       -0.634326         0.366618   1.361704   \n",
       "394209                       -0.565600         0.366618  -0.734374   \n",
       "...                                ...              ...        ...   \n",
       "509444                       -0.053064         0.366618   1.361704   \n",
       "389557                       -0.398868         0.366618  -0.734374   \n",
       "582253                       -0.226416         0.366618   1.361704   \n",
       "670183                       -0.164511         0.366618  -0.734374   \n",
       "257272                       -0.285170         0.366618  -0.734374   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "334972        -0.334593     -1.364472    0.0  \n",
       "477554        -0.334593     -1.364472    0.0  \n",
       "129226        -0.334593      0.732884    0.0  \n",
       "166812        -0.334593     -1.364472    0.0  \n",
       "394209        -0.334593     -1.364472    0.0  \n",
       "...                 ...           ...    ...  \n",
       "509444        -0.334593      0.732884    0.0  \n",
       "389557        -0.334593     -1.364472    0.0  \n",
       "582253        -0.334593     -1.364472    0.0  \n",
       "670183        -0.334593      0.732884    0.0  \n",
       "257272        -0.334593     -1.364472    0.0  \n",
       "\n",
       "[69960 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_fraud_undersample = resample(no_fraud, \n",
    "                                    replace=True, \n",
    "                                    n_samples = len(fraud),\n",
    "                                    random_state=0)\n",
    "no_fraud_undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334972</th>\n",
       "      <td>-0.324630</td>\n",
       "      <td>0.202841</td>\n",
       "      <td>-0.082387</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477554</th>\n",
       "      <td>-0.255016</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.251904</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129226</th>\n",
       "      <td>-0.384474</td>\n",
       "      <td>-0.131608</td>\n",
       "      <td>-0.588554</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166812</th>\n",
       "      <td>0.102985</td>\n",
       "      <td>-0.182768</td>\n",
       "      <td>-0.634326</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>1.361704</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394209</th>\n",
       "      <td>-0.339862</td>\n",
       "      <td>-0.186961</td>\n",
       "      <td>-0.565600</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799964</th>\n",
       "      <td>1.897907</td>\n",
       "      <td>-0.102498</td>\n",
       "      <td>-0.069320</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799972</th>\n",
       "      <td>-0.382112</td>\n",
       "      <td>-0.113786</td>\n",
       "      <td>1.799053</td>\n",
       "      <td>-2.727632</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>-1.364472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799984</th>\n",
       "      <td>0.194263</td>\n",
       "      <td>-0.186387</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799987</th>\n",
       "      <td>-0.191712</td>\n",
       "      <td>-0.185819</td>\n",
       "      <td>0.835812</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799992</th>\n",
       "      <td>1.163050</td>\n",
       "      <td>-0.186513</td>\n",
       "      <td>-0.113849</td>\n",
       "      <td>0.366618</td>\n",
       "      <td>-0.734374</td>\n",
       "      <td>-0.334593</td>\n",
       "      <td>0.732884</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139920 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "334972           -0.324630                        0.202841   \n",
       "477554           -0.255016                       -0.136678   \n",
       "129226           -0.384474                       -0.131608   \n",
       "166812            0.102985                       -0.182768   \n",
       "394209           -0.339862                       -0.186961   \n",
       "...                    ...                             ...   \n",
       "799964            1.897907                       -0.102498   \n",
       "799972           -0.382112                       -0.113786   \n",
       "799984            0.194263                       -0.186387   \n",
       "799987           -0.191712                       -0.185819   \n",
       "799992            1.163050                       -0.186513   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "334972                       -0.082387         0.366618  -0.734374   \n",
       "477554                       -0.251904         0.366618   1.361704   \n",
       "129226                       -0.588554        -2.727632  -0.734374   \n",
       "166812                       -0.634326         0.366618   1.361704   \n",
       "394209                       -0.565600         0.366618  -0.734374   \n",
       "...                                ...              ...        ...   \n",
       "799964                       -0.069320         0.366618  -0.734374   \n",
       "799972                        1.799053        -2.727632  -0.734374   \n",
       "799984                        0.965774         0.366618  -0.734374   \n",
       "799987                        0.835812         0.366618  -0.734374   \n",
       "799992                       -0.113849         0.366618  -0.734374   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "334972        -0.334593     -1.364472    0.0  \n",
       "477554        -0.334593     -1.364472    0.0  \n",
       "129226        -0.334593      0.732884    0.0  \n",
       "166812        -0.334593     -1.364472    0.0  \n",
       "394209        -0.334593     -1.364472    0.0  \n",
       "...                 ...           ...    ...  \n",
       "799964        -0.334593      0.732884    1.0  \n",
       "799972        -0.334593     -1.364472    1.0  \n",
       "799984        -0.334593      0.732884    1.0  \n",
       "799987        -0.334593      0.732884    1.0  \n",
       "799992        -0.334593      0.732884    1.0  \n",
       "\n",
       "[139920 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_under = pd.concat([no_fraud_undersample, fraud])\n",
    "train_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fraud_plt = train_under[\"fraud\"].value_counts()\n",
    "fraud_plt.plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating our features from our target\n",
    "\n",
    "X_train_under = train_under.drop(columns = [\"fraud\"])\n",
    "y_train_under = train_under[\"fraud\"]\n",
    "\n",
    "#Model training\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96    182557\n",
      "         1.0       0.58      0.95      0.72     17443\n",
      "\n",
      "    accuracy                           0.93    200000\n",
      "   macro avg       0.79      0.94      0.84    200000\n",
      "weighted avg       0.96      0.93      0.94    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our model\n",
    "\n",
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? \n",
    "\n",
    "sm = SMOTE(random_state = 1,sampling_strategy=1.0)\n",
    "\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train_scaled,y_train)\n",
    "\n",
    "# train our model\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96    182557\n",
      "         1.0       0.58      0.95      0.72     17443\n",
      "\n",
      "    accuracy                           0.93    200000\n",
      "   macro avg       0.79      0.94      0.84    200000\n",
      "weighted avg       0.96      0.93      0.94    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our model\n",
    "\n",
    "pred = log_reg.predict(X_test_scaled)\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
